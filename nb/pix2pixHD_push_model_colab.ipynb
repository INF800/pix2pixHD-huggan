{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pixHD-push-model-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install huggingface_hub huggingface\n",
        "\n",
        "# Install git-lfs, as it's required to push to hub\n",
        "! apt install git-lfs\n",
        "\n",
        "# ðŸš¨ - Replace with your git info!\n",
        "# ...or you can remove and manually pass git_email and git_username to push_to_hub\n",
        "! git config --global user.name \"INF800\"\n",
        "! git config --global user.email \"rakeshark22@gmail.com\"\n",
        "\n",
        "# Enable credential helper used by huggingface-cli\n",
        "! git config --global credential.helper store"
      ],
      "metadata": {
        "id": "JVZbXYKsRmJ4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqnujHB2O6MZ",
        "outputId": "9583101e-98bd-461f-d67d-5e1876801452"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens.\n",
            "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_card_template.md\n",
        "\n",
        "---\n",
        "tags:\n",
        "- huggan\n",
        "- gan\n",
        "# See a list of available tags here:\n",
        "# https://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Types.ts#L12\n",
        "# task: unconditional-image-generation or conditional-image-generation or image-to-image\n",
        "license: mit\n",
        "---\n",
        "\n",
        "# MyModelName\n",
        "\n",
        "## Model description\n",
        "\n",
        "Describe the model here (what it does, what it's used for, etc.)\n",
        "\n",
        "## Intended uses & limitations\n",
        "\n",
        "#### How to use\n",
        "\n",
        "```python\n",
        "# You can include sample code which will be formatted\n",
        "```\n",
        "\n",
        "#### Limitations and bias\n",
        "\n",
        "Provide examples of latent issues and potential remediations.\n",
        "\n",
        "## Training data\n",
        "\n",
        "Describe the data you used to train the model.\n",
        "If you initialized it with pre-trained weights, add a link to the pre-trained model card or repository with description of the pre-training data.\n",
        "\n",
        "## Training procedure\n",
        "\n",
        "Preprocessing, hardware used, hyperparameters...\n",
        "\n",
        "## Eval results\n",
        "\n",
        "## Generated Images\n",
        "\n",
        "You can embed local or remote images using `![](...)`\n",
        "\n",
        "### BibTeX entry and citation info\n",
        "\n",
        "```bibtex\n",
        "@inproceedings{...,\n",
        "  year={2020}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlBuili3LVyD",
        "outputId": "36c31e09-fd34-4a49-a1d6-b3df219d7a41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_card_template.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile huggan_modelhub_mixin.py\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from re import TEMPLATE\n",
        "from typing import Optional, Union\n",
        "\n",
        "from huggingface_hub import PyTorchModelHubMixin, HfFolder, Repository, HfApi\n",
        "\n",
        "TEMPLATE_MODEL_CARD_PATH = Path('/content/model_card_template.md')\n",
        "\n",
        "\n",
        "class HugGANModelHubMixin(PyTorchModelHubMixin):\n",
        "    \"\"\"A mixin to push PyTorch Models to the Hugging Face Hub. This\n",
        "    mixin was adapted from the PyTorchModelHubMixin to also push a template\n",
        "    README.md for the HugGAN sprint.\n",
        "    \"\"\"\n",
        "\n",
        "    def push_to_hub(\n",
        "        self,\n",
        "        repo_path_or_name: Optional[str] = None,\n",
        "        repo_url: Optional[str] = None,\n",
        "        commit_message: Optional[str] = \"Add model\",\n",
        "        organization: Optional[str] = None,\n",
        "        private: Optional[bool] = None,\n",
        "        api_endpoint: Optional[str] = None,\n",
        "        use_auth_token: Optional[Union[bool, str]] = None,\n",
        "        git_user: Optional[str] = None,\n",
        "        git_email: Optional[str] = None,\n",
        "        config: Optional[dict] = None,\n",
        "        default_model_card: Optional[str] = TEMPLATE_MODEL_CARD_PATH\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Upload model checkpoint or tokenizer files to the Hub while\n",
        "        synchronizing a local clone of the repo in `repo_path_or_name`.\n",
        "        Parameters:\n",
        "            repo_path_or_name (`str`, *optional*):\n",
        "                Can either be a repository name for your model or tokenizer in\n",
        "                the Hub or a path to a local folder (in which case the\n",
        "                repository will have the name of that local folder). If not\n",
        "                specified, will default to the name given by `repo_url` and a\n",
        "                local directory with that name will be created.\n",
        "            repo_url (`str`, *optional*):\n",
        "                Specify this in case you want to push to an existing repository\n",
        "                in the hub. If unspecified, a new repository will be created in\n",
        "                your namespace (unless you specify an `organization`) with\n",
        "                `repo_name`.\n",
        "            commit_message (`str`, *optional*):\n",
        "                Message to commit while pushing. Will default to `\"add config\"`,\n",
        "                `\"add tokenizer\"` or `\"add model\"` depending on the type of the\n",
        "                class.\n",
        "            organization (`str`, *optional*):\n",
        "                Organization in which you want to push your model or tokenizer\n",
        "                (you must be a member of this organization).\n",
        "            private (`bool`, *optional*):\n",
        "                Whether the repository created should be private.\n",
        "            api_endpoint (`str`, *optional*):\n",
        "                The API endpoint to use when pushing the model to the hub.\n",
        "            use_auth_token (`bool` or `str`, *optional*):\n",
        "                The token to use as HTTP bearer authorization for remote files.\n",
        "                If `True`, will use the token generated when running\n",
        "                `transformers-cli login` (stored in `~/.huggingface`). Will\n",
        "                default to `True` if `repo_url` is not specified.\n",
        "            git_user (`str`, *optional*):\n",
        "                will override the `git config user.name` for committing and\n",
        "                pushing files to the hub.\n",
        "            git_email (`str`, *optional*):\n",
        "                will override the `git config user.email` for committing and\n",
        "                pushing files to the hub.\n",
        "            config (`dict`, *optional*):\n",
        "                Configuration object to be saved alongside the model weights.\n",
        "            default_model_card (`str`, *optional*):\n",
        "                Path to a markdown file to use as your default model card.\n",
        "        Returns:\n",
        "            The url of the commit of your model in the given repository.\n",
        "        \"\"\"\n",
        "\n",
        "        if repo_path_or_name is None and repo_url is None:\n",
        "            raise ValueError(\n",
        "                \"You need to specify a `repo_path_or_name` or a `repo_url`.\"\n",
        "            )\n",
        "\n",
        "        if use_auth_token is None and repo_url is None:\n",
        "            token = HfFolder.get_token()\n",
        "            if token is None:\n",
        "                raise ValueError(\n",
        "                    \"You must login to the Hugging Face hub on this computer by typing `huggingface-cli login` and \"\n",
        "                    \"entering your credentials to use `use_auth_token=True`. Alternatively, you can pass your own \"\n",
        "                    \"token as the `use_auth_token` argument.\"\n",
        "                )\n",
        "        elif isinstance(use_auth_token, str):\n",
        "            token = use_auth_token\n",
        "        else:\n",
        "            token = None\n",
        "\n",
        "        if repo_path_or_name is None:\n",
        "            repo_path_or_name = repo_url.split(\"/\")[-1]\n",
        "\n",
        "        # If no URL is passed and there's no path to a directory containing files, create a repo\n",
        "        if repo_url is None and not os.path.exists(repo_path_or_name):\n",
        "            repo_id = Path(repo_path_or_name).name\n",
        "            if organization:\n",
        "                repo_id = f\"{organization}/{repo_id}\"\n",
        "            repo_url = HfApi(endpoint=api_endpoint).create_repo(\n",
        "                repo_id=repo_id,\n",
        "                token=token,\n",
        "                private=private,\n",
        "                repo_type=None,\n",
        "                exist_ok=True,\n",
        "            )\n",
        "\n",
        "        repo = Repository(\n",
        "            repo_path_or_name,\n",
        "            clone_from=repo_url,\n",
        "            use_auth_token=use_auth_token,\n",
        "            git_user=git_user,\n",
        "            git_email=git_email,\n",
        "        )\n",
        "        repo.git_pull(rebase=True)\n",
        "\n",
        "        # Save the files in the cloned repo\n",
        "        self.save_pretrained(repo_path_or_name, config=config)\n",
        "\n",
        "        model_card_path = Path(repo_path_or_name) / TEMPLATE_MODEL_CARD_PATH\n",
        "        if not model_card_path.exists():\n",
        "            model_card_path.write_text(TEMPLATE_MODEL_CARD_PATH.read_text())\n",
        "\n",
        "        # Commit and push!\n",
        "        repo.git_add()\n",
        "        repo.git_commit(commit_message)\n",
        "        return repo.git_push()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMTsRGsXKzk4",
        "outputId": "92042c96-d64b-4fe7-a16c-03854cac7556"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing huggan_modelhub_mixin.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile options.py\n",
        "\"\"\" training settings/args/options \"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "\n",
        "#### UTILS\n",
        "\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "\n",
        "#### BASE OPTIONS\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self):    \n",
        "        # experiment specifics\n",
        "        self.parser.add_argument('--name', type=str, default='label2city', help='name of the experiment. It decides where to store samples and models')        \n",
        "        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "        self.parser.add_argument('--model', type=str, default='pix2pixHD', help='which model to use')\n",
        "        self.parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization')        \n",
        "        self.parser.add_argument('--use_dropout', action='store_true', help='use dropout for the generator')\n",
        "        self.parser.add_argument('--data_type', default=32, type=int, choices=[8, 16, 32], help=\"Supported data type i.e. 8, 16, 32 bit\")\n",
        "        self.parser.add_argument('--verbose', action='store_true', default=False, help='toggles verbose')\n",
        "        self.parser.add_argument('--fp16', action='store_true', default=False, help='train with AMP')\n",
        "        self.parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')\n",
        "\n",
        "        # input/output sizes       \n",
        "        self.parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n",
        "        self.parser.add_argument('--loadSize', type=int, default=1024, help='scale images to this size')\n",
        "        self.parser.add_argument('--fineSize', type=int, default=512, help='then crop to this size')\n",
        "        self.parser.add_argument('--label_nc', type=int, default=35, help='# of input label channels')\n",
        "        self.parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels')\n",
        "        self.parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels')\n",
        "\n",
        "        # for setting inputs\n",
        "        self.parser.add_argument('--dataroot', type=str, default='./datasets/cityscapes/') \n",
        "        self.parser.add_argument('--resize_or_crop', type=str, default='scale_width', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]')\n",
        "        self.parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')        \n",
        "        self.parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data argumentation') \n",
        "        self.parser.add_argument('--nThreads', default=2, type=int, help='# threads for loading data')                \n",
        "        self.parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
        "\n",
        "        # for displays\n",
        "        self.parser.add_argument('--display_winsize', type=int, default=512,  help='display window size')\n",
        "        self.parser.add_argument('--tf_log', action='store_true', help='if specified, use tensorboard logging. Requires tensorflow installed')\n",
        "\n",
        "        # for generator\n",
        "        self.parser.add_argument('--netG', type=str, default='global', help='selects model to use for netG')\n",
        "        self.parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
        "        self.parser.add_argument('--n_downsample_global', type=int, default=4, help='number of downsampling layers in netG') \n",
        "        self.parser.add_argument('--n_blocks_global', type=int, default=9, help='number of residual blocks in the global generator network')\n",
        "        self.parser.add_argument('--n_blocks_local', type=int, default=3, help='number of residual blocks in the local enhancer network')\n",
        "        self.parser.add_argument('--n_local_enhancers', type=int, default=1, help='number of local enhancers to use')        \n",
        "        self.parser.add_argument('--niter_fix_global', type=int, default=0, help='number of epochs that we only train the outmost local enhancer')        \n",
        "\n",
        "        # for instance-wise features\n",
        "        self.parser.add_argument('--no_instance', action='store_true', help='if specified, do *not* add instance map as input')        \n",
        "        self.parser.add_argument('--instance_feat', action='store_true', help='if specified, add encoded instance features as input')\n",
        "        self.parser.add_argument('--label_feat', action='store_true', help='if specified, add encoded label features as input')        \n",
        "        self.parser.add_argument('--feat_num', type=int, default=3, help='vector length for encoded features')        \n",
        "        self.parser.add_argument('--load_features', action='store_true', help='if specified, load precomputed feature maps')\n",
        "        self.parser.add_argument('--n_downsample_E', type=int, default=4, help='# of downsampling layers in encoder') \n",
        "        self.parser.add_argument('--nef', type=int, default=16, help='# of encoder filters in the first conv layer')        \n",
        "        self.parser.add_argument('--n_clusters', type=int, default=10, help='number of clusters for features')        \n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def parse(self, save=True):\n",
        "        if not self.initialized:\n",
        "            self.initialize()\n",
        "        self.opt = self.parser.parse_args()\n",
        "        self.opt.isTrain = self.isTrain   # train or test\n",
        "\n",
        "        str_ids = self.opt.gpu_ids.split(',')\n",
        "        self.opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                self.opt.gpu_ids.append(id)\n",
        "        \n",
        "        # set gpu ids\n",
        "        if len(self.opt.gpu_ids) > 0:\n",
        "            torch.cuda.set_device(self.opt.gpu_ids[0])\n",
        "\n",
        "        args = vars(self.opt)\n",
        "\n",
        "        print('------------ Options -------------')\n",
        "        for k, v in sorted(args.items()):\n",
        "            print('%s: %s' % (str(k), str(v)))\n",
        "        print('-------------- End ----------------')\n",
        "\n",
        "        # save to the disk        \n",
        "        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n",
        "        mkdirs(expr_dir)\n",
        "        if save and not self.opt.continue_train:\n",
        "            file_name = os.path.join(expr_dir, 'opt.txt')\n",
        "            with open(file_name, 'wt') as opt_file:\n",
        "                opt_file.write('------------ Options -------------\\n')\n",
        "                for k, v in sorted(args.items()):\n",
        "                    opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
        "                opt_file.write('-------------- End ----------------\\n')\n",
        "        return self.opt\n",
        "\n",
        "\n",
        "class TrainOptions(BaseOptions):\n",
        "    def initialize(self):\n",
        "        BaseOptions.initialize(self)\n",
        "        # for displays\n",
        "        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n",
        "        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
        "        self.parser.add_argument('--save_latest_freq', type=int, default=1000, help='frequency of saving the latest results')\n",
        "        self.parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')        \n",
        "        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
        "        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n",
        "\n",
        "        # for training\n",
        "        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        self.parser.add_argument('--load_pretrain', type=str, default='', help='load the pretrained model from the specified location')\n",
        "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n",
        "        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n",
        "        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
        "        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
        "\n",
        "        # for discriminators        \n",
        "        self.parser.add_argument('--num_D', type=int, default=2, help='number of discriminators to use')\n",
        "        self.parser.add_argument('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\n",
        "        self.parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')    \n",
        "        self.parser.add_argument('--lambda_feat', type=float, default=10.0, help='weight for feature matching loss')                \n",
        "        self.parser.add_argument('--no_ganFeat_loss', action='store_true', help='if specified, do *not* use discriminator feature matching loss')\n",
        "        self.parser.add_argument('--no_vgg_loss', action='store_true', help='if specified, do *not* use VGG feature matching loss')        \n",
        "        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n",
        "        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n",
        "\n",
        "        self.isTrain = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h51kBcb-L5Q_",
        "outputId": "12ef3b0f-0a8c-4c0c-c0c6-57e7a9dd1f28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing options.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQSUKTzlDLtk",
        "outputId": "498c4b45-abd1-4fb4-e2cd-dcd2af0eeacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tmp.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tmp.py\n",
        "\n",
        "\"\"\"Model used for training and inference. Heavily relies on NVIDIA's original implementation\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# from huggan.pytorch.huggan_mixin import HugGANModelHubMixin\n",
        "# from huggan.pytorch.pix2pixHD.options import TrainOptions\n",
        "from huggan_modelhub_mixin import HugGANModelHubMixin\n",
        "from options import TrainOptions\n",
        "\n",
        "\n",
        "#### UTIL - IMAGEPOOL\n",
        "\n",
        "class ImagePool():\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool_size = pool_size\n",
        "        if self.pool_size > 0:\n",
        "            self.num_imgs = 0\n",
        "            self.images = []\n",
        "\n",
        "    def query(self, images):\n",
        "        if self.pool_size == 0:\n",
        "            return images\n",
        "        return_images = []\n",
        "        for image in images.data:\n",
        "            image = torch.unsqueeze(image, 0)\n",
        "            if self.num_imgs < self.pool_size:\n",
        "                self.num_imgs = self.num_imgs + 1\n",
        "                self.images.append(image)\n",
        "                return_images.append(image)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:\n",
        "                    random_id = random.randint(0, self.pool_size-1)\n",
        "                    tmp = self.images[random_id].clone()\n",
        "                    self.images[random_id] = image\n",
        "                    return_images.append(tmp)\n",
        "                else:\n",
        "                    return_images.append(image)\n",
        "        return_images = Variable(torch.cat(return_images, 0))\n",
        "        return return_images\n",
        "\n",
        "\n",
        "#### UTIL - TENSOR2{IMAGE|LABEL} \n",
        "\n",
        "# Converts a Tensor into a Numpy array\n",
        "# |imtype|: the desired type of the converted numpy array\n",
        "def tensor2im(image_tensor, imtype=np.uint8, normalize=True):\n",
        "    if isinstance(image_tensor, list):\n",
        "        image_numpy = []\n",
        "        for i in range(len(image_tensor)):\n",
        "            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n",
        "        return image_numpy\n",
        "    image_numpy = image_tensor.cpu().float().numpy()\n",
        "    if normalize:\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    else:\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0      \n",
        "    image_numpy = np.clip(image_numpy, 0, 255)\n",
        "    if image_numpy.shape[2] == 1 or image_numpy.shape[2] > 3:        \n",
        "        image_numpy = image_numpy[:,:,0]\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "# Converts a one-hot tensor into a colorful label map\n",
        "def tensor2label(label_tensor, n_label, imtype=np.uint8):\n",
        "    if n_label == 0:\n",
        "        return tensor2im(label_tensor, imtype)\n",
        "    label_tensor = label_tensor.cpu().float()    \n",
        "    if label_tensor.size()[0] > 1:\n",
        "        label_tensor = label_tensor.max(0, keepdim=True)[1]\n",
        "    label_tensor = Colorize(n_label)(label_tensor)\n",
        "    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n",
        "    return label_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "\n",
        "#### NETWORKS RELATED STUFF\n",
        "\n",
        "# Functions\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def get_norm_layer(norm_type='instance'):\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "def define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1, \n",
        "             n_blocks_local=3, norm='instance', gpu_ids=[]):    \n",
        "    norm_layer = get_norm_layer(norm_type=norm)     \n",
        "    if netG == 'global':    \n",
        "        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)       \n",
        "    elif netG == 'local':        \n",
        "        netG = LocalEnhancer(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, \n",
        "                                  n_local_enhancers, n_blocks_local, norm_layer)\n",
        "    elif netG == 'encoder':\n",
        "        netG = Encoder(input_nc, output_nc, ngf, n_downsample_global, norm_layer)\n",
        "    else:\n",
        "        raise('generator not implemented!')\n",
        "    print(netG)\n",
        "    if len(gpu_ids) > 0:\n",
        "        assert(torch.cuda.is_available())   \n",
        "        netG.cuda(gpu_ids[0])\n",
        "    netG.apply(weights_init)\n",
        "    return netG\n",
        "\n",
        "def define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1, getIntermFeat=False, gpu_ids=[]):        \n",
        "    norm_layer = get_norm_layer(norm_type=norm)   \n",
        "    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)   \n",
        "    print(netD)\n",
        "    if len(gpu_ids) > 0:\n",
        "        assert(torch.cuda.is_available())\n",
        "        netD.cuda(gpu_ids[0])\n",
        "    netD.apply(weights_init)\n",
        "    return netD\n",
        "\n",
        "def print_network(net):\n",
        "    if isinstance(net, list):\n",
        "        net = net[0]\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "# Losses\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
        "                 tensor=torch.FloatTensor):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.real_label = target_real_label\n",
        "        self.fake_label = target_fake_label\n",
        "        self.real_label_var = None\n",
        "        self.fake_label_var = None\n",
        "        self.Tensor = tensor\n",
        "        if use_lsgan:\n",
        "            self.loss = nn.MSELoss()\n",
        "        else:\n",
        "            self.loss = nn.BCELoss()\n",
        "\n",
        "    def get_target_tensor(self, input, target_is_real):\n",
        "        target_tensor = None\n",
        "        if target_is_real:\n",
        "            create_label = ((self.real_label_var is None) or\n",
        "                            (self.real_label_var.numel() != input.numel()))\n",
        "            if create_label:\n",
        "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
        "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
        "            target_tensor = self.real_label_var\n",
        "        else:\n",
        "            create_label = ((self.fake_label_var is None) or\n",
        "                            (self.fake_label_var.numel() != input.numel()))\n",
        "            if create_label:\n",
        "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
        "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
        "            target_tensor = self.fake_label_var\n",
        "        return target_tensor\n",
        "\n",
        "    def __call__(self, input, target_is_real):\n",
        "        if isinstance(input[0], list):\n",
        "            loss = 0\n",
        "            for input_i in input:\n",
        "                pred = input_i[-1]\n",
        "                target_tensor = self.get_target_tensor(pred, target_is_real)\n",
        "                loss += self.loss(pred, target_tensor)\n",
        "            return loss\n",
        "        else:            \n",
        "            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n",
        "            return self.loss(input[-1], target_tensor)\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, gpu_ids):\n",
        "        super(VGGLoss, self).__init__()        \n",
        "        self.vgg = Vgg19().cuda()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n",
        "\n",
        "    def forward(self, x, y):              \n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = 0\n",
        "        for i in range(len(x_vgg)):\n",
        "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n",
        "        return loss\n",
        "\n",
        "# Generator\n",
        "class LocalEnhancer(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=32, n_downsample_global=3, n_blocks_global=9, \n",
        "                 n_local_enhancers=1, n_blocks_local=3, norm_layer=nn.BatchNorm2d, padding_type='reflect'):        \n",
        "        super(LocalEnhancer, self).__init__()\n",
        "        self.n_local_enhancers = n_local_enhancers\n",
        "        \n",
        "        ###### global generator model #####           \n",
        "        ngf_global = ngf * (2**n_local_enhancers)\n",
        "        model_global = GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model        \n",
        "        model_global = [model_global[i] for i in range(len(model_global)-3)] # get rid of final convolution layers        \n",
        "        self.model = nn.Sequential(*model_global)                \n",
        "\n",
        "        ###### local enhancer layers #####\n",
        "        for n in range(1, n_local_enhancers+1):\n",
        "            ### downsample            \n",
        "            ngf_global = ngf * (2**(n_local_enhancers-n))\n",
        "            model_downsample = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size=7, padding=0), \n",
        "                                norm_layer(ngf_global), nn.ReLU(True),\n",
        "                                nn.Conv2d(ngf_global, ngf_global * 2, kernel_size=3, stride=2, padding=1), \n",
        "                                norm_layer(ngf_global * 2), nn.ReLU(True)]\n",
        "            ### residual blocks\n",
        "            model_upsample = []\n",
        "            for i in range(n_blocks_local):\n",
        "                model_upsample += [ResnetBlock(ngf_global * 2, padding_type=padding_type, norm_layer=norm_layer)]\n",
        "\n",
        "            ### upsample\n",
        "            model_upsample += [nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               norm_layer(ngf_global), nn.ReLU(True)]      \n",
        "\n",
        "            ### final convolution\n",
        "            if n == n_local_enhancers:                \n",
        "                model_upsample += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]                       \n",
        "            \n",
        "            setattr(self, 'model'+str(n)+'_1', nn.Sequential(*model_downsample))\n",
        "            setattr(self, 'model'+str(n)+'_2', nn.Sequential(*model_upsample))                  \n",
        "        \n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def forward(self, input): \n",
        "        ### create input pyramid\n",
        "        input_downsampled = [input]\n",
        "        for i in range(self.n_local_enhancers):\n",
        "            input_downsampled.append(self.downsample(input_downsampled[-1]))\n",
        "\n",
        "        ### output at coarest level\n",
        "        output_prev = self.model(input_downsampled[-1])        \n",
        "        ### build up one layer at a time\n",
        "        for n_local_enhancers in range(1, self.n_local_enhancers+1):\n",
        "            model_downsample = getattr(self, 'model'+str(n_local_enhancers)+'_1')\n",
        "            model_upsample = getattr(self, 'model'+str(n_local_enhancers)+'_2')            \n",
        "            input_i = input_downsampled[self.n_local_enhancers-n_local_enhancers]            \n",
        "            output_prev = model_upsample(model_downsample(input_i) + output_prev)\n",
        "        return output_prev\n",
        "\n",
        "class GlobalGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n",
        "                 padding_type='reflect'):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(GlobalGenerator, self).__init__()        \n",
        "        activation = nn.ReLU(True)        \n",
        "\n",
        "        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n",
        "        ### downsample\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n",
        "                      norm_layer(ngf * mult * 2), activation]\n",
        "\n",
        "        ### resnet blocks\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n",
        "        \n",
        "        ### upsample         \n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                       norm_layer(int(ngf * mult / 2)), activation]\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]        \n",
        "        self.model = nn.Sequential(*model)\n",
        "            \n",
        "    def forward(self, input):\n",
        "        return self.model(input)             \n",
        "        \n",
        "# Define a resnet block\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n",
        "                       norm_layer(dim),\n",
        "                       activation]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=32, n_downsampling=4, norm_layer=nn.BatchNorm2d):\n",
        "        super(Encoder, self).__init__()        \n",
        "        self.output_nc = output_nc        \n",
        "\n",
        "        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), \n",
        "                 norm_layer(ngf), nn.ReLU(True)]             \n",
        "        ### downsample\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n",
        "                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n",
        "\n",
        "        ### upsample         \n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                       norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]        \n",
        "\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n",
        "        self.model = nn.Sequential(*model) \n",
        "\n",
        "    def forward(self, input, inst):\n",
        "        outputs = self.model(input)\n",
        "\n",
        "        # instance-wise average pooling\n",
        "        outputs_mean = outputs.clone()\n",
        "        inst_list = np.unique(inst.cpu().numpy().astype(int))        \n",
        "        for i in inst_list:\n",
        "            for b in range(input.size()[0]):\n",
        "                indices = (inst[b:b+1] == int(i)).nonzero() # n x 4            \n",
        "                for j in range(self.output_nc):\n",
        "                    output_ins = outputs[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]]                    \n",
        "                    mean_feat = torch.mean(output_ins).expand_as(output_ins)                                        \n",
        "                    outputs_mean[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]] = mean_feat                       \n",
        "        return outputs_mean\n",
        "\n",
        "class MultiscaleDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
        "                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n",
        "        super(MultiscaleDiscriminator, self).__init__()\n",
        "        self.num_D = num_D\n",
        "        self.n_layers = n_layers\n",
        "        self.getIntermFeat = getIntermFeat\n",
        "     \n",
        "        for i in range(num_D):\n",
        "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
        "            if getIntermFeat:                                \n",
        "                for j in range(n_layers+2):\n",
        "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
        "            else:\n",
        "                setattr(self, 'layer'+str(i), netD.model)\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def singleD_forward(self, model, input):\n",
        "        if self.getIntermFeat:\n",
        "            result = [input]\n",
        "            for i in range(len(model)):\n",
        "                result.append(model[i](result[-1]))\n",
        "            return result[1:]\n",
        "        else:\n",
        "            return [model(input)]\n",
        "\n",
        "    def forward(self, input):        \n",
        "        num_D = self.num_D\n",
        "        result = []\n",
        "        input_downsampled = input\n",
        "        for i in range(num_D):\n",
        "            if self.getIntermFeat:\n",
        "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
        "            else:\n",
        "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
        "            result.append(self.singleD_forward(model, input_downsampled))\n",
        "            if i != (num_D-1):\n",
        "                input_downsampled = self.downsample(input_downsampled)\n",
        "        return result\n",
        "        \n",
        "# Defines the PatchGAN discriminator with the specified arguments.\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        self.getIntermFeat = getIntermFeat\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        kw = 4\n",
        "        padw = int(np.ceil((kw-1.0)/2))\n",
        "        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n",
        "\n",
        "        nf = ndf\n",
        "        for n in range(1, n_layers):\n",
        "            nf_prev = nf\n",
        "            nf = min(nf * 2, 512)\n",
        "            sequence += [[\n",
        "                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n",
        "                norm_layer(nf), nn.LeakyReLU(0.2, True)\n",
        "            ]]\n",
        "\n",
        "        nf_prev = nf\n",
        "        nf = min(nf * 2, 512)\n",
        "        sequence += [[\n",
        "            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n",
        "            norm_layer(nf),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]]\n",
        "\n",
        "        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n",
        "\n",
        "        if use_sigmoid:\n",
        "            sequence += [[nn.Sigmoid()]]\n",
        "\n",
        "        if getIntermFeat:\n",
        "            for n in range(len(sequence)):\n",
        "                setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n",
        "        else:\n",
        "            sequence_stream = []\n",
        "            for n in range(len(sequence)):\n",
        "                sequence_stream += sequence[n]\n",
        "            self.model = nn.Sequential(*sequence_stream)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.getIntermFeat:\n",
        "            res = [input]\n",
        "            for n in range(self.n_layers+2):\n",
        "                model = getattr(self, 'model'+str(n))\n",
        "                res.append(model(res[-1]))\n",
        "            return res[1:]\n",
        "        else:\n",
        "            return self.model(input)        \n",
        "\n",
        "class Vgg19(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg19, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)        \n",
        "        h_relu3 = self.slice3(h_relu2)        \n",
        "        h_relu4 = self.slice4(h_relu3)        \n",
        "        h_relu5 = self.slice5(h_relu4)                \n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "#### BASE MODEL\n",
        "\n",
        "class BaseModel(torch.nn.Module):\n",
        "    def name(self):\n",
        "        return 'BaseModel'\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        self.opt = opt\n",
        "        self.gpu_ids = opt.gpu_ids\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "\n",
        "    def set_input(self, input):\n",
        "        self.input = input\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    # used in test time, no backprop\n",
        "    def test(self):\n",
        "        pass\n",
        "\n",
        "    def get_image_paths(self):\n",
        "        pass\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def get_current_visuals(self):\n",
        "        return self.input\n",
        "\n",
        "    def get_current_errors(self):\n",
        "        return {}\n",
        "\n",
        "    def save(self, label):\n",
        "        pass\n",
        "\n",
        "    # helper saving function that can be used by subclasses\n",
        "    def save_network(self, network, network_label, epoch_label, gpu_ids):\n",
        "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
        "        save_path = os.path.join(self.save_dir, save_filename)\n",
        "        torch.save(network.cpu().state_dict(), save_path)\n",
        "        if len(gpu_ids) and torch.cuda.is_available():\n",
        "            network.cuda()\n",
        "\n",
        "    # helper loading function that can be used by subclasses\n",
        "    def load_network(self, network, network_label, epoch_label, save_dir=''):        \n",
        "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
        "        if not save_dir:\n",
        "            save_dir = self.save_dir\n",
        "        save_path = os.path.join(save_dir, save_filename)        \n",
        "        if not os.path.isfile(save_path):\n",
        "            print('%s not exists yet!' % save_path)\n",
        "            if network_label == 'G':\n",
        "                raise('Generator must exist!')\n",
        "        else:\n",
        "            #network.load_state_dict(torch.load(save_path))\n",
        "            try:\n",
        "                network.load_state_dict(torch.load(save_path))\n",
        "            except:   \n",
        "                pretrained_dict = torch.load(save_path)                \n",
        "                model_dict = network.state_dict()\n",
        "                try:\n",
        "                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                    \n",
        "                    network.load_state_dict(pretrained_dict)\n",
        "                    if self.opt.verbose:\n",
        "                        print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n",
        "                except:\n",
        "                    print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n",
        "                    for k, v in pretrained_dict.items():                      \n",
        "                        if v.size() == model_dict[k].size():\n",
        "                            model_dict[k] = v\n",
        "\n",
        "                    if sys.version_info >= (3,0):\n",
        "                        not_initialized = set()\n",
        "                    else:\n",
        "                        from sets import Set\n",
        "                        not_initialized = Set()                    \n",
        "\n",
        "                    for k, v in model_dict.items():\n",
        "                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n",
        "                            not_initialized.add(k.split('.')[0])\n",
        "                    \n",
        "                    print(sorted(not_initialized))\n",
        "                    network.load_state_dict(model_dict)                  \n",
        "\n",
        "    def update_learning_rate():\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "#### PIX2PIXHD, INFERENCE MODEL\n",
        "\n",
        "class Pix2PixHDModel(BaseModel, HugGANModelHubMixin):\n",
        "    def name(self):\n",
        "        return 'Pix2PixHDModel'\n",
        "    \n",
        "    def init_loss_filter(self, use_gan_feat_loss, use_vgg_loss):\n",
        "        flags = (True, use_gan_feat_loss, use_vgg_loss, True, True)\n",
        "        def loss_filter(g_gan, g_gan_feat, g_vgg, d_real, d_fake):\n",
        "            return [l for (l,f) in zip((g_gan,g_gan_feat,g_vgg,d_real,d_fake),flags) if f]\n",
        "        return loss_filter\n",
        "    \n",
        "    def initialize(self, opt):\n",
        "        BaseModel.initialize(self, opt)\n",
        "        if opt.resize_or_crop != 'none' or not opt.isTrain: # when training at full res this causes OOM\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.use_features = opt.instance_feat or opt.label_feat\n",
        "        self.gen_features = self.use_features and not self.opt.load_features\n",
        "        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n",
        "\n",
        "        ##### define networks        \n",
        "        # Generator network\n",
        "        netG_input_nc = input_nc        \n",
        "        if not opt.no_instance:\n",
        "            netG_input_nc += 1\n",
        "        if self.use_features:\n",
        "            netG_input_nc += opt.feat_num                  \n",
        "        self.netG = define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
        "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
        "                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n",
        "\n",
        "        # Discriminator network\n",
        "        if self.isTrain:\n",
        "            use_sigmoid = opt.no_lsgan\n",
        "            netD_input_nc = input_nc + opt.output_nc\n",
        "            if not opt.no_instance:\n",
        "                netD_input_nc += 1\n",
        "            self.netD = define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
        "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n",
        "\n",
        "        ### Encoder network\n",
        "        if self.gen_features:          \n",
        "            self.netE = define_G(opt.output_nc, opt.feat_num, opt.nef, 'encoder', \n",
        "                                          opt.n_downsample_E, norm=opt.norm, gpu_ids=self.gpu_ids)  \n",
        "        if self.opt.verbose:\n",
        "                print('---------- Networks initialized -------------')\n",
        "\n",
        "        # load networks\n",
        "        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n",
        "            pretrained_path = '' if not self.isTrain else opt.load_pretrain\n",
        "            self.load_network(self.netG, 'G', opt.which_epoch, pretrained_path)            \n",
        "            if self.isTrain:\n",
        "                self.load_network(self.netD, 'D', opt.which_epoch, pretrained_path)  \n",
        "            if self.gen_features:\n",
        "                self.load_network(self.netE, 'E', opt.which_epoch, pretrained_path)              \n",
        "\n",
        "        # set loss functions and optimizers\n",
        "        if self.isTrain:\n",
        "            if opt.pool_size > 0 and (len(self.gpu_ids)) > 1:\n",
        "                raise NotImplementedError(\"Fake Pool Not Implemented for MultiGPU\")\n",
        "            self.fake_pool = ImagePool(opt.pool_size)\n",
        "            self.old_lr = opt.lr\n",
        "\n",
        "            # define loss functions\n",
        "            self.loss_filter = self.init_loss_filter(not opt.no_ganFeat_loss, not opt.no_vgg_loss)\n",
        "            \n",
        "            self.criterionGAN = GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)   \n",
        "            self.criterionFeat = torch.nn.L1Loss()\n",
        "            if not opt.no_vgg_loss:             \n",
        "                self.criterionVGG = VGGLoss(self.gpu_ids)\n",
        "                \n",
        "        \n",
        "            # Names so we can breakout loss\n",
        "            self.loss_names = self.loss_filter('G_GAN','G_GAN_Feat','G_VGG','D_real', 'D_fake')\n",
        "\n",
        "            # initialize optimizers\n",
        "            # optimizer G\n",
        "            if opt.niter_fix_global > 0:                \n",
        "                import sys\n",
        "                if sys.version_info >= (3,0):\n",
        "                    finetune_list = set()\n",
        "                else:\n",
        "                    from sets import Set\n",
        "                    finetune_list = Set()\n",
        "\n",
        "                params_dict = dict(self.netG.named_parameters())\n",
        "                params = []\n",
        "                for key, value in params_dict.items():       \n",
        "                    if key.startswith('model' + str(opt.n_local_enhancers)):                    \n",
        "                        params += [value]\n",
        "                        finetune_list.add(key.split('.')[0])  \n",
        "                print('------------- Only training the local enhancer network (for %d epochs) ------------' % opt.niter_fix_global)\n",
        "                print('The layers that are finetuned are ', sorted(finetune_list))                         \n",
        "            else:\n",
        "                params = list(self.netG.parameters())\n",
        "            if self.gen_features:              \n",
        "                params += list(self.netE.parameters())         \n",
        "            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n",
        "\n",
        "            # optimizer D                        \n",
        "            params = list(self.netD.parameters())    \n",
        "            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "\n",
        "    def encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, infer=False):             \n",
        "        if self.opt.label_nc == 0:\n",
        "            input_label = label_map.data.cuda()\n",
        "        else:\n",
        "            # create one-hot vector for label map \n",
        "            size = label_map.size()\n",
        "            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n",
        "            input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n",
        "            input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n",
        "            if self.opt.data_type == 16:\n",
        "                input_label = input_label.half()\n",
        "\n",
        "        # get edges from instance map\n",
        "        if not self.opt.no_instance:\n",
        "            inst_map = inst_map.data.cuda()\n",
        "            edge_map = self.get_edges(inst_map)\n",
        "            input_label = torch.cat((input_label, edge_map), dim=1)         \n",
        "        input_label = Variable(input_label, volatile=infer)\n",
        "\n",
        "        # real images for training\n",
        "        if real_image is not None:\n",
        "            real_image = Variable(real_image.data.cuda())\n",
        "\n",
        "        # instance map for feature encoding\n",
        "        if self.use_features:\n",
        "            # get precomputed feature maps\n",
        "            if self.opt.load_features:\n",
        "                feat_map = Variable(feat_map.data.cuda())\n",
        "            if self.opt.label_feat:\n",
        "                inst_map = label_map.cuda()\n",
        "\n",
        "        return input_label, inst_map, real_image, feat_map\n",
        "\n",
        "    def discriminate(self, input_label, test_image, use_pool=False):\n",
        "        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n",
        "        if use_pool:            \n",
        "            fake_query = self.fake_pool.query(input_concat)\n",
        "            return self.netD.forward(fake_query)\n",
        "        else:\n",
        "            return self.netD.forward(input_concat)\n",
        "\n",
        "    def forward(self, label, inst, image, feat, infer=False):\n",
        "        # Encode Inputs\n",
        "        input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)  \n",
        "\n",
        "        # Fake Generation\n",
        "        if self.use_features:\n",
        "            if not self.opt.load_features:\n",
        "                feat_map = self.netE.forward(real_image, inst_map)                     \n",
        "            input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
        "        else:\n",
        "            input_concat = input_label\n",
        "        fake_image = self.netG.forward(input_concat)\n",
        "\n",
        "        # Fake Detection and Loss\n",
        "        pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n",
        "        loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n",
        "\n",
        "        # Real Detection and Loss        \n",
        "        pred_real = self.discriminate(input_label, real_image)\n",
        "        loss_D_real = self.criterionGAN(pred_real, True)\n",
        "\n",
        "        # GAN loss (Fake Passability Loss)        \n",
        "        pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n",
        "        loss_G_GAN = self.criterionGAN(pred_fake, True)               \n",
        "        \n",
        "        # GAN feature matching loss\n",
        "        loss_G_GAN_Feat = 0\n",
        "        if not self.opt.no_ganFeat_loss:\n",
        "            feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n",
        "            D_weights = 1.0 / self.opt.num_D\n",
        "            for i in range(self.opt.num_D):\n",
        "                for j in range(len(pred_fake[i])-1):\n",
        "                    loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
        "                        self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n",
        "                   \n",
        "        # VGG feature matching loss\n",
        "        loss_G_VGG = 0\n",
        "        if not self.opt.no_vgg_loss:\n",
        "            loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n",
        "        \n",
        "        # Only return the fake_B image if necessary to save BW\n",
        "        return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake ), None if not infer else fake_image ]\n",
        "\n",
        "    def inference(self, label, inst, image=None):\n",
        "        # Encode Inputs        \n",
        "        image = Variable(image) if image is not None else None\n",
        "        input_label, inst_map, real_image, _ = self.encode_input(Variable(label), Variable(inst), image, infer=True)\n",
        "\n",
        "        # Fake Generation\n",
        "        if self.use_features:\n",
        "            if self.opt.use_encoded_image:\n",
        "                # encode the real image to get feature map\n",
        "                feat_map = self.netE.forward(real_image, inst_map)\n",
        "            else:\n",
        "                # sample clusters from precomputed features             \n",
        "                feat_map = self.sample_features(inst_map)\n",
        "            input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
        "        else:\n",
        "            input_concat = input_label        \n",
        "           \n",
        "        if torch.__version__.startswith('0.4'):\n",
        "            with torch.no_grad():\n",
        "                fake_image = self.netG.forward(input_concat)\n",
        "        else:\n",
        "            fake_image = self.netG.forward(input_concat)\n",
        "        return fake_image\n",
        "\n",
        "    def sample_features(self, inst): \n",
        "        # read precomputed feature clusters \n",
        "        cluster_path = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.cluster_path)        \n",
        "        features_clustered = np.load(cluster_path, encoding='latin1').item()\n",
        "\n",
        "        # randomly sample from the feature clusters\n",
        "        inst_np = inst.cpu().numpy().astype(int)                                      \n",
        "        feat_map = self.Tensor(inst.size()[0], self.opt.feat_num, inst.size()[2], inst.size()[3])\n",
        "        for i in np.unique(inst_np):    \n",
        "            label = i if i < 1000 else i//1000\n",
        "            if label in features_clustered:\n",
        "                feat = features_clustered[label]\n",
        "                cluster_idx = np.random.randint(0, feat.shape[0]) \n",
        "                                            \n",
        "                idx = (inst == int(i)).nonzero()\n",
        "                for k in range(self.opt.feat_num):                                    \n",
        "                    feat_map[idx[:,0], idx[:,1] + k, idx[:,2], idx[:,3]] = feat[cluster_idx, k]\n",
        "        if self.opt.data_type==16:\n",
        "            feat_map = feat_map.half()\n",
        "        return feat_map\n",
        "\n",
        "    def encode_features(self, image, inst):\n",
        "        image = Variable(image.cuda(), volatile=True)\n",
        "        feat_num = self.opt.feat_num\n",
        "        h, w = inst.size()[2], inst.size()[3]\n",
        "        block_num = 32\n",
        "        feat_map = self.netE.forward(image, inst.cuda())\n",
        "        inst_np = inst.cpu().numpy().astype(int)\n",
        "        feature = {}\n",
        "        for i in range(self.opt.label_nc):\n",
        "            feature[i] = np.zeros((0, feat_num+1))\n",
        "        for i in np.unique(inst_np):\n",
        "            label = i if i < 1000 else i//1000\n",
        "            idx = (inst == int(i)).nonzero()\n",
        "            num = idx.size()[0]\n",
        "            idx = idx[num//2,:]\n",
        "            val = np.zeros((1, feat_num+1))                        \n",
        "            for k in range(feat_num):\n",
        "                val[0, k] = feat_map[idx[0], idx[1] + k, idx[2], idx[3]].data[0]            \n",
        "            val[0, feat_num] = float(num) / (h * w // block_num)\n",
        "            feature[label] = np.append(feature[label], val, axis=0)\n",
        "        return feature\n",
        "\n",
        "    def get_edges(self, t):\n",
        "        edge = torch.cuda.ByteTensor(t.size()).zero_()\n",
        "        edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
        "        edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
        "        edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
        "        edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
        "        if self.opt.data_type==16:\n",
        "            return edge.half()\n",
        "        else:\n",
        "            return edge.float()\n",
        "\n",
        "    def save(self, which_epoch):\n",
        "        self.save_network(self.netG, 'G', which_epoch, self.gpu_ids)\n",
        "        self.save_network(self.netD, 'D', which_epoch, self.gpu_ids)\n",
        "        if self.gen_features:\n",
        "            self.save_network(self.netE, 'E', which_epoch, self.gpu_ids)\n",
        "\n",
        "    def update_fixed_params(self):\n",
        "        # after fixing the global generator for a number of iterations, also start finetuning it\n",
        "        params = list(self.netG.parameters())\n",
        "        if self.gen_features:\n",
        "            params += list(self.netE.parameters())           \n",
        "        self.optimizer_G = torch.optim.Adam(params, lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n",
        "        if self.opt.verbose:\n",
        "            print('------------ Now also finetuning global generator -----------')\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        lrd = self.opt.lr / self.opt.niter_decay\n",
        "        lr = self.old_lr - lrd        \n",
        "        for param_group in self.optimizer_D.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        for param_group in self.optimizer_G.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        if self.opt.verbose:\n",
        "            print('update learning rate: %f -> %f' % (self.old_lr, lr))\n",
        "        self.old_lr = lr\n",
        "\n",
        "class InferenceModel(Pix2PixHDModel):\n",
        "    def forward(self, inp):\n",
        "        label, inst = inp\n",
        "        return self.inference(label, inst)\n",
        "\n",
        "\n",
        "        \n",
        "#### UI MODEL\n",
        "\n",
        "class UIModel(BaseModel):\n",
        "    def name(self):\n",
        "        return 'UIModel'\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        assert(not opt.isTrain)\n",
        "        BaseModel.initialize(self, opt)\n",
        "        self.use_features = opt.instance_feat or opt.label_feat\n",
        "\n",
        "        netG_input_nc = opt.label_nc\n",
        "        if not opt.no_instance:\n",
        "            netG_input_nc += 1            \n",
        "        if self.use_features:   \n",
        "            netG_input_nc += opt.feat_num           \n",
        "\n",
        "        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
        "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
        "                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)            \n",
        "        self.load_network(self.netG, 'G', opt.which_epoch)\n",
        "\n",
        "        print('---------- Networks initialized -------------')\n",
        "\n",
        "    def toTensor(self, img, normalize=False):\n",
        "        tensor = torch.from_numpy(np.array(img, np.int32, copy=False))\n",
        "        tensor = tensor.view(1, img.size[1], img.size[0], len(img.mode))    \n",
        "        tensor = tensor.transpose(1, 2).transpose(1, 3).contiguous()\n",
        "        if normalize:\n",
        "            return (tensor.float()/255.0 - 0.5) / 0.5        \n",
        "        return tensor.float()\n",
        "\n",
        "    def load_image(self, label_path, inst_path, feat_path):\n",
        "        opt = self.opt\n",
        "        # read label map\n",
        "        label_img = Image.open(label_path)    \n",
        "        if label_path.find('face') != -1:\n",
        "            label_img = label_img.convert('L')\n",
        "        ow, oh = label_img.size    \n",
        "        w = opt.loadSize\n",
        "        h = int(w * oh / ow)    \n",
        "        label_img = label_img.resize((w, h), Image.NEAREST)\n",
        "        label_map = self.toTensor(label_img)           \n",
        "        \n",
        "        # onehot vector input for label map\n",
        "        self.label_map = label_map.cuda()\n",
        "        oneHot_size = (1, opt.label_nc, h, w)\n",
        "        input_label = self.Tensor(torch.Size(oneHot_size)).zero_()\n",
        "        self.input_label = input_label.scatter_(1, label_map.long().cuda(), 1.0)\n",
        "\n",
        "        # read instance map\n",
        "        if not opt.no_instance:\n",
        "            inst_img = Image.open(inst_path)        \n",
        "            inst_img = inst_img.resize((w, h), Image.NEAREST)            \n",
        "            self.inst_map = self.toTensor(inst_img).cuda()\n",
        "            self.edge_map = self.get_edges(self.inst_map)          \n",
        "            self.net_input = Variable(torch.cat((self.input_label, self.edge_map), dim=1), volatile=True)\n",
        "        else:\n",
        "            self.net_input = Variable(self.input_label, volatile=True)  \n",
        "        \n",
        "        self.features_clustered = np.load(feat_path).item()\n",
        "        self.object_map = self.inst_map if opt.instance_feat else self.label_map \n",
        "                       \n",
        "        object_np = self.object_map.cpu().numpy().astype(int) \n",
        "        self.feat_map = self.Tensor(1, opt.feat_num, h, w).zero_()                 \n",
        "        self.cluster_indices = np.zeros(self.opt.label_nc, np.uint8)\n",
        "        for i in np.unique(object_np):    \n",
        "            label = i if i < 1000 else i//1000\n",
        "            if label in self.features_clustered:\n",
        "                feat = self.features_clustered[label]\n",
        "                np.random.seed(i+1)\n",
        "                cluster_idx = np.random.randint(0, feat.shape[0])\n",
        "                self.cluster_indices[label] = cluster_idx\n",
        "                idx = (self.object_map == i).nonzero()                    \n",
        "                self.set_features(idx, feat, cluster_idx)\n",
        "\n",
        "        self.net_input_original = self.net_input.clone()        \n",
        "        self.label_map_original = self.label_map.clone()\n",
        "        self.feat_map_original = self.feat_map.clone()\n",
        "        if not opt.no_instance:\n",
        "            self.inst_map_original = self.inst_map.clone()        \n",
        "\n",
        "    def reset(self):\n",
        "        self.net_input = self.net_input_prev = self.net_input_original.clone()        \n",
        "        self.label_map = self.label_map_prev = self.label_map_original.clone()\n",
        "        self.feat_map = self.feat_map_prev = self.feat_map_original.clone()\n",
        "        if not self.opt.no_instance:\n",
        "            self.inst_map = self.inst_map_prev = self.inst_map_original.clone()\n",
        "        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map \n",
        "\n",
        "    def undo(self):        \n",
        "        self.net_input = self.net_input_prev\n",
        "        self.label_map = self.label_map_prev\n",
        "        self.feat_map = self.feat_map_prev\n",
        "        if not self.opt.no_instance:\n",
        "            self.inst_map = self.inst_map_prev\n",
        "        self.object_map = self.inst_map if self.opt.instance_feat else self.label_map \n",
        "            \n",
        "    # get boundary map from instance map\n",
        "    def get_edges(self, t):\n",
        "        edge = torch.cuda.ByteTensor(t.size()).zero_()\n",
        "        edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
        "        edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
        "        edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
        "        edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
        "        return edge.float()\n",
        "\n",
        "    # change the label at the source position to the label at the target position\n",
        "    def change_labels(self, click_src, click_tgt): \n",
        "        y_src, x_src = click_src[0], click_src[1]\n",
        "        y_tgt, x_tgt = click_tgt[0], click_tgt[1]\n",
        "        label_src = int(self.label_map[0, 0, y_src, x_src])\n",
        "        inst_src = self.inst_map[0, 0, y_src, x_src]\n",
        "        label_tgt = int(self.label_map[0, 0, y_tgt, x_tgt])\n",
        "        inst_tgt = self.inst_map[0, 0, y_tgt, x_tgt]\n",
        "\n",
        "        idx_src = (self.inst_map == inst_src).nonzero()         \n",
        "        # need to change 3 things: label map, instance map, and feature map\n",
        "        if idx_src.shape:\n",
        "            # backup current maps\n",
        "            self.backup_current_state() \n",
        "\n",
        "            # change both the label map and the network input\n",
        "            self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n",
        "            self.net_input[idx_src[:,0], idx_src[:,1] + label_src, idx_src[:,2], idx_src[:,3]] = 0\n",
        "            self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1                                    \n",
        "            \n",
        "            # update the instance map (and the network input)\n",
        "            if inst_tgt > 1000:\n",
        "                # if different instances have different ids, give the new object a new id\n",
        "                tgt_indices = (self.inst_map > label_tgt * 1000) & (self.inst_map < (label_tgt+1) * 1000)\n",
        "                inst_tgt = self.inst_map[tgt_indices].max() + 1\n",
        "            self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = inst_tgt\n",
        "            self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n",
        "\n",
        "            # also copy the source features to the target position      \n",
        "            idx_tgt = (self.inst_map == inst_tgt).nonzero()    \n",
        "            if idx_tgt.shape:\n",
        "                self.copy_features(idx_src, idx_tgt[0,:])\n",
        "\n",
        "        self.fake_image = tensor2im(self.single_forward(self.net_input, self.feat_map))\n",
        "\n",
        "    # add strokes of target label in the image\n",
        "    def add_strokes(self, click_src, label_tgt, bw, save):\n",
        "        # get the region of the new strokes (bw is the brush width)        \n",
        "        size = self.net_input.size()\n",
        "        h, w = size[2], size[3]\n",
        "        idx_src = torch.LongTensor(bw**2, 4).fill_(0)\n",
        "        for i in range(bw):\n",
        "            idx_src[i*bw:(i+1)*bw, 2] = min(h-1, max(0, click_src[0]-bw//2 + i))\n",
        "            for j in range(bw):\n",
        "                idx_src[i*bw+j, 3] = min(w-1, max(0, click_src[1]-bw//2 + j))\n",
        "        idx_src = idx_src.cuda()\n",
        "        \n",
        "        # again, need to update 3 things\n",
        "        if idx_src.shape:\n",
        "            # backup current maps\n",
        "            if save:\n",
        "                self.backup_current_state()\n",
        "\n",
        "            # update the label map (and the network input) in the stroke region            \n",
        "            self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n",
        "            for k in range(self.opt.label_nc):\n",
        "                self.net_input[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = 0\n",
        "            self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1                 \n",
        "\n",
        "            # update the instance map (and the network input)\n",
        "            self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n",
        "            self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n",
        "            \n",
        "            # also update the features if available\n",
        "            if self.opt.instance_feat:                                            \n",
        "                feat = self.features_clustered[label_tgt]\n",
        "                #np.random.seed(label_tgt+1)   \n",
        "                #cluster_idx = np.random.randint(0, feat.shape[0])\n",
        "                cluster_idx = self.cluster_indices[label_tgt]\n",
        "                self.set_features(idx_src, feat, cluster_idx)                                                  \n",
        "        \n",
        "        self.fake_image = tensor2im(self.single_forward(self.net_input, self.feat_map))\n",
        "\n",
        "    # add an object to the clicked position with selected style\n",
        "    def add_objects(self, click_src, label_tgt, mask, style_id=0):\n",
        "        y, x = click_src[0], click_src[1]\n",
        "        mask = np.transpose(mask, (2, 0, 1))[np.newaxis,...]        \n",
        "        idx_src = torch.from_numpy(mask).cuda().nonzero()        \n",
        "        idx_src[:,2] += y\n",
        "        idx_src[:,3] += x\n",
        "\n",
        "        # backup current maps\n",
        "        self.backup_current_state()\n",
        "\n",
        "        # update label map\n",
        "        self.label_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt        \n",
        "        for k in range(self.opt.label_nc):\n",
        "            self.net_input[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = 0\n",
        "        self.net_input[idx_src[:,0], idx_src[:,1] + label_tgt, idx_src[:,2], idx_src[:,3]] = 1            \n",
        "\n",
        "        # update instance map\n",
        "        self.inst_map[idx_src[:,0], idx_src[:,1], idx_src[:,2], idx_src[:,3]] = label_tgt\n",
        "        self.net_input[:,-1,:,:] = self.get_edges(self.inst_map)\n",
        "                \n",
        "        # update feature map\n",
        "        self.set_features(idx_src, self.feat, style_id)                \n",
        "        \n",
        "        self.fake_image = tensor2im(self.single_forward(self.net_input, self.feat_map))\n",
        "\n",
        "    def single_forward(self, net_input, feat_map):\n",
        "        net_input = torch.cat((net_input, feat_map), dim=1)\n",
        "        fake_image = self.netG.forward(net_input)\n",
        "\n",
        "        if fake_image.size()[0] == 1:\n",
        "            return fake_image.data[0]        \n",
        "        return fake_image.data\n",
        "\n",
        "\n",
        "    # generate all outputs for different styles\n",
        "    def style_forward(self, click_pt, style_id=-1):           \n",
        "        if click_pt is None:            \n",
        "            self.fake_image = tensor2im(self.single_forward(self.net_input, self.feat_map))\n",
        "            self.crop = None\n",
        "            self.mask = None        \n",
        "        else:                       \n",
        "            instToChange = int(self.object_map[0, 0, click_pt[0], click_pt[1]])\n",
        "            self.instToChange = instToChange\n",
        "            label = instToChange if instToChange < 1000 else instToChange//1000        \n",
        "            self.feat = self.features_clustered[label]\n",
        "            self.fake_image = []\n",
        "            self.mask = self.object_map == instToChange\n",
        "            idx = self.mask.nonzero()\n",
        "            self.get_crop_region(idx)            \n",
        "            if idx.size():                \n",
        "                if style_id == -1:\n",
        "                    (min_y, min_x, max_y, max_x) = self.crop\n",
        "                    ### original\n",
        "                    for cluster_idx in range(self.opt.multiple_output):\n",
        "                        self.set_features(idx, self.feat, cluster_idx)\n",
        "                        fake_image = self.single_forward(self.net_input, self.feat_map)\n",
        "                        fake_image = tensor2im(fake_image[:,min_y:max_y,min_x:max_x])\n",
        "                        self.fake_image.append(fake_image)    \n",
        "                    \"\"\"### To speed up previewing different style results, either crop or downsample the label maps\n",
        "                    if instToChange > 1000:\n",
        "                        (min_y, min_x, max_y, max_x) = self.crop                                                \n",
        "                        ### crop                                                \n",
        "                        _, _, h, w = self.net_input.size()\n",
        "                        offset = 512\n",
        "                        y_start, x_start = max(0, min_y-offset), max(0, min_x-offset)\n",
        "                        y_end, x_end = min(h, (max_y + offset)), min(w, (max_x + offset))\n",
        "                        y_region = slice(y_start, y_start+(y_end-y_start)//16*16)\n",
        "                        x_region = slice(x_start, x_start+(x_end-x_start)//16*16)\n",
        "                        net_input = self.net_input[:,:,y_region,x_region]                    \n",
        "                        for cluster_idx in range(self.opt.multiple_output):  \n",
        "                            self.set_features(idx, self.feat, cluster_idx)\n",
        "                            fake_image = self.single_forward(net_input, self.feat_map[:,:,y_region,x_region])                            \n",
        "                            fake_image = tensor2im(fake_image[:,min_y-y_start:max_y-y_start,min_x-x_start:max_x-x_start])\n",
        "                            self.fake_image.append(fake_image)\n",
        "                    else:\n",
        "                        ### downsample\n",
        "                        (min_y, min_x, max_y, max_x) = [crop//2 for crop in self.crop]                    \n",
        "                        net_input = self.net_input[:,:,::2,::2]                    \n",
        "                        size = net_input.size()\n",
        "                        net_input_batch = net_input.expand(self.opt.multiple_output, size[1], size[2], size[3])             \n",
        "                        for cluster_idx in range(self.opt.multiple_output):  \n",
        "                            self.set_features(idx, self.feat, cluster_idx)\n",
        "                            feat_map = self.feat_map[:,:,::2,::2]\n",
        "                            if cluster_idx == 0:\n",
        "                                feat_map_batch = feat_map\n",
        "                            else:\n",
        "                                feat_map_batch = torch.cat((feat_map_batch, feat_map), dim=0)\n",
        "                        fake_image_batch = self.single_forward(net_input_batch, feat_map_batch)\n",
        "                        for i in range(self.opt.multiple_output):\n",
        "                            self.fake_image.append(tensor2im(fake_image_batch[i,:,min_y:max_y,min_x:max_x]))\"\"\"\n",
        "                                        \n",
        "                else:\n",
        "                    self.set_features(idx, self.feat, style_id)\n",
        "                    self.cluster_indices[label] = style_id\n",
        "                    self.fake_image = tensor2im(self.single_forward(self.net_input, self.feat_map))        \n",
        "\n",
        "    def backup_current_state(self):\n",
        "        self.net_input_prev = self.net_input.clone()\n",
        "        self.label_map_prev = self.label_map.clone() \n",
        "        self.inst_map_prev = self.inst_map.clone() \n",
        "        self.feat_map_prev = self.feat_map.clone() \n",
        "\n",
        "    # crop the ROI and get the mask of the object\n",
        "    def get_crop_region(self, idx):\n",
        "        size = self.net_input.size()\n",
        "        h, w = size[2], size[3]\n",
        "        min_y, min_x = idx[:,2].min(), idx[:,3].min()\n",
        "        max_y, max_x = idx[:,2].max(), idx[:,3].max()             \n",
        "        crop_min = 128\n",
        "        if max_y - min_y < crop_min:\n",
        "            min_y = max(0, (max_y + min_y) // 2 - crop_min // 2)\n",
        "            max_y = min(h-1, min_y + crop_min)\n",
        "        if max_x - min_x < crop_min:\n",
        "            min_x = max(0, (max_x + min_x) // 2 - crop_min // 2)\n",
        "            max_x = min(w-1, min_x + crop_min)\n",
        "        self.crop = (min_y, min_x, max_y, max_x)           \n",
        "        self.mask = self.mask[:,:, min_y:max_y, min_x:max_x]\n",
        "\n",
        "    # update the feature map once a new object is added or the label is changed\n",
        "    def update_features(self, cluster_idx, mask=None, click_pt=None):        \n",
        "        self.feat_map_prev = self.feat_map.clone()\n",
        "        # adding a new object\n",
        "        if mask is not None:\n",
        "            y, x = click_pt[0], click_pt[1]\n",
        "            mask = np.transpose(mask, (2,0,1))[np.newaxis,...]        \n",
        "            idx = torch.from_numpy(mask).cuda().nonzero()        \n",
        "            idx[:,2] += y\n",
        "            idx[:,3] += x    \n",
        "        # changing the label of an existing object \n",
        "        else:            \n",
        "            idx = (self.object_map == self.instToChange).nonzero()              \n",
        "\n",
        "        # update feature map\n",
        "        self.set_features(idx, self.feat, cluster_idx)        \n",
        "\n",
        "    # set the class features to the target feature\n",
        "    def set_features(self, idx, feat, cluster_idx):        \n",
        "        for k in range(self.opt.feat_num):\n",
        "            self.feat_map[idx[:,0], idx[:,1] + k, idx[:,2], idx[:,3]] = feat[cluster_idx, k] \n",
        "\n",
        "    # copy the features at the target position to the source position\n",
        "    def copy_features(self, idx_src, idx_tgt):        \n",
        "        for k in range(self.opt.feat_num):\n",
        "            val = self.feat_map[idx_tgt[0], idx_tgt[1] + k, idx_tgt[2], idx_tgt[3]]\n",
        "            self.feat_map[idx_src[:,0], idx_src[:,1] + k, idx_src[:,2], idx_src[:,3]] = val \n",
        "\n",
        "    def get_current_visuals(self, getLabel=False):                              \n",
        "        mask = self.mask     \n",
        "        if self.mask is not None:\n",
        "            mask = np.transpose(self.mask[0].cpu().float().numpy(), (1,2,0)).astype(np.uint8)        \n",
        "\n",
        "        dict_list = [('fake_image', self.fake_image), ('mask', mask)]\n",
        "\n",
        "        if getLabel: # only output label map if needed to save bandwidth\n",
        "            label = tensor2label(self.net_input.data[0], self.opt.label_nc)                    \n",
        "            dict_list += [('label', label)]\n",
        "\n",
        "        return OrderedDict(dict_list)\n",
        "\n",
        "\n",
        "\n",
        "#### CREATE MODEL\n",
        "def create_model(opt):\n",
        "    if opt.model == 'pix2pixHD':\n",
        "        if opt.isTrain:\n",
        "            model = Pix2PixHDModel()\n",
        "        else:\n",
        "            model = InferenceModel()\n",
        "    else:\n",
        "    \tmodel = UIModel()\n",
        "    model.initialize(opt)\n",
        "    if opt.verbose:\n",
        "        print(\"model [%s] was created\" % (model.name()))\n",
        "\n",
        "    if opt.isTrain and len(opt.gpu_ids) and not opt.fp16:\n",
        "        model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#### TEST PUSHING TO HUB\n",
        "def test_push_model():\n",
        "    opt = TrainOptions().parse()\n",
        "    model = Pix2PixHDModel()\n",
        "    model.initialize(opt)\n",
        "    if opt.verbose:\n",
        "        print(\"model [%s] was created\" % (model.name()))\n",
        "\n",
        "    if opt.isTrain and len(opt.gpu_ids) and not opt.fp16:\n",
        "        model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\n",
        "\n",
        "    model.module.push_to_hub('huggan/test1223')\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    test_push_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git init\n",
        "# !git add .\n",
        "# !git commit -m'xyz;'\n"
      ],
      "metadata": {
        "id": "W4zqsjUDRQ9D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add args\n",
        "!python tmp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xA9_L4OKetw",
        "outputId": "732ff617-5042-4a43-8c15-f0b7bc851330"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Options -------------\n",
            "batchSize: 1\n",
            "beta1: 0.5\n",
            "checkpoints_dir: ./checkpoints\n",
            "continue_train: False\n",
            "data_type: 32\n",
            "dataroot: ./datasets/cityscapes/\n",
            "debug: False\n",
            "display_freq: 100\n",
            "display_winsize: 512\n",
            "feat_num: 3\n",
            "fineSize: 512\n",
            "fp16: False\n",
            "gpu_ids: [0]\n",
            "input_nc: 3\n",
            "instance_feat: False\n",
            "isTrain: True\n",
            "label_feat: False\n",
            "label_nc: 35\n",
            "lambda_feat: 10.0\n",
            "loadSize: 1024\n",
            "load_features: False\n",
            "load_pretrain: \n",
            "local_rank: 0\n",
            "lr: 0.0002\n",
            "max_dataset_size: inf\n",
            "model: pix2pixHD\n",
            "nThreads: 2\n",
            "n_blocks_global: 9\n",
            "n_blocks_local: 3\n",
            "n_clusters: 10\n",
            "n_downsample_E: 4\n",
            "n_downsample_global: 4\n",
            "n_layers_D: 3\n",
            "n_local_enhancers: 1\n",
            "name: label2city\n",
            "ndf: 64\n",
            "nef: 16\n",
            "netG: global\n",
            "ngf: 64\n",
            "niter: 100\n",
            "niter_decay: 100\n",
            "niter_fix_global: 0\n",
            "no_flip: False\n",
            "no_ganFeat_loss: False\n",
            "no_html: False\n",
            "no_instance: False\n",
            "no_lsgan: False\n",
            "no_vgg_loss: False\n",
            "norm: instance\n",
            "num_D: 2\n",
            "output_nc: 3\n",
            "phase: train\n",
            "pool_size: 0\n",
            "print_freq: 100\n",
            "resize_or_crop: scale_width\n",
            "save_epoch_freq: 10\n",
            "save_latest_freq: 1000\n",
            "serial_batches: False\n",
            "tf_log: False\n",
            "use_dropout: False\n",
            "verbose: False\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "GlobalGenerator(\n",
            "  (model): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(36, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (17): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (18): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (19): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (20): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (21): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (22): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (23): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (24): ResnetBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
            "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (30): ReLU(inplace=True)\n",
            "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (39): Tanh()\n",
            "  )\n",
            ")\n",
            "MultiscaleDiscriminator(\n",
            "  (scale0_layer0): Sequential(\n",
            "    (0): Conv2d(39, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale0_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (scale1_layer0): Sequential(\n",
            "    (0): Conv2d(39, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (scale1_layer4): Sequential(\n",
            "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "  )\n",
            "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
            ")\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:03<00:00, 165MB/s]\n",
            "Cloning https://huggingface.co/arakesh/test1223 into local empty directory.\n",
            "Upload file pytorch_model.bin: 100% 767M/767M [11:09<00:00, 1.27MB/s]To https://huggingface.co/arakesh/test1223\n",
            "   29f96c5..350c2ae  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 767M/767M [11:11<00:00, 1.20MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no model card appears..."
      ],
      "metadata": {
        "id": "tseCgr8ATUSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}